{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import utils\n",
    "import scipy.stats\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "try:\n",
    "    from linear_model import BayesianLinearModel\n",
    "except:\n",
    "    msg = \"'The 'BayesianLinearModel' module does not appear to be installed.\"\n",
    "    raise Exception(msg)\n",
    "\n",
    "# Use same random data (for reproducibility).\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Bayesian Linear Regression\n",
    "\n",
    "**Authors**: Asher Bender   \n",
    "$$\n",
    "\\DeclareMathOperator{\\absoluteVal}{abs}\n",
    "\\renewcommand{\\det} [1]{{\\begin{vmatrix}#1\\end{vmatrix}}}                             \n",
    "\\newcommand{\\brac}  [1]{{ \\left( #1 \\right) }}\n",
    "\\newcommand{\\sbrac} [1]{{ \\left[ #1 \\right] }}\n",
    "\\newcommand{\\cbrac} [1]{{ \\left{ #1 \\right} }}\n",
    "\\newcommand{\\AutoMiddle}{{\\;\\vert\\;}}\n",
    "\\newcommand{\\prob}      [2][p]{{#1\\!\\brac{#2}}}\n",
    "\\newcommand{\\condprob}  [2]{p\\!\\brac{#1\\AutoMiddle#2}}\n",
    "\\newcommand{\\observations}{{\\mathcal{D}}}\n",
    "\\newcommand{\\sinput}{{x}}\n",
    "\\newcommand{\\vinput}{{\\mathbf{x}}}\n",
    "\\newcommand{\\minput}{{\\mathbf{X}}}\n",
    "\\newcommand{\\soutput}{{y}}\n",
    "\\newcommand{\\voutput}{{\\mathbf{y}}}\n",
    "\\newcommand{\\moutput}{{\\mathbf{Y}}}\n",
    "\\newcommand{\\starget}{{t}}\n",
    "\\newcommand{\\vtarget}{{\\mathbf{t}}}\n",
    "\\newcommand{\\mtarget}{{\\mathbf{T}}}\n",
    "\\newcommand{\\weight}{{w}}\n",
    "\\newcommand{\\weights}{{\\mathbf{\\weight}}}\n",
    "\\newcommand{\\params}{{\\mathbf{\\theta}}}\n",
    "\\newcommand{\\design}{{\\Phi}}\n",
    "\\newcommand{\\sbasis}{{\\phi}}\n",
    "\\newcommand{\\vbasis}{{\\mathbf{\\phi}}}\n",
    "\\newcommand{\\snoise}{{\\epsilon}}\n",
    "\\newcommand{\\vnoise}{{\\mathbf{\\snoise}}}\n",
    "\\newcommand{\\sbasisfcn}[1]{\\sbasis\\!\\brac{#1}}\n",
    "\\newcommand{\\vbasisfcn}[1]{\\vbasis\\!\\brac{#1}}\n",
    "\\newcommand{\\variance}{{\\sigma^2}}\n",
    "\\newcommand{\\precision}{{\\lambda}}\n",
    "\\newcommand{\\normal}{{\\mathcal{N}}}\n",
    "\\newcommand{\\normalmean}{{\\mathbf{\\mu}}}\n",
    "\\newcommand{\\normalvariance}{{\\mathbf{V}}}\n",
    "\\newcommand{\\normalprecision}{{\\mathbf{S}}}\n",
    "\\newcommand{\\normaldist}[2]{\\normal\\brac{#1, #2}}\n",
    "\\newcommand{\\normalcond}[3]{\\normal\\brac{#1\\AutoMiddle#2, #3}}\n",
    "\\newcommand{\\gammashape}{{\\alpha}}\n",
    "\\newcommand{\\gammascale}{{\\beta}}\n",
    "\\newcommand{\\gammarate}{{\\beta}}\n",
    "\\newcommand{\\gammasym}{{\\mathcal{G}}}\n",
    "\\newcommand{\\gammadist}[2]{\\gammasym\\brac{#1, #2}}\n",
    "\\newcommand{\\gammacond}[3]{\\gammasym\\brac{#1\\AutoMiddle#2, #3}}\n",
    "\\newcommand{\\igammadist}[2]{i\\gammasym\\brac{#1, #2}}\n",
    "\\newcommand{\\igammacond}[3]{i\\gammasym\\brac{#1\\AutoMiddle#2, #3}}\n",
    "\\newcommand{\\normaligammadist}[4]{\\normal\\!i\\gammasym\\brac{#1, #2, #3, #4}}\n",
    "\\newcommand{\\normaligammacond}[5]{\\normal\\!i\\gammasym\\brac{#1\\AutoMiddle#2, #3, #4, #5}}\n",
    "\\newcommand{\\normalgammadist}[4]{\\normal\\gammasym\\brac{#1, #2, #3, #4}}\n",
    "\\newcommand{\\normalgammacond}[5]{\\normal\\gammasym\\brac{#1\\AutoMiddle#2, #3, #4, #5}}\n",
    "\\newcommand{\\gammafcn}[1]{\\Gamma\\!\\brac{#1}}\n",
    "\\newcommand{\\data}{\\mathcal{D}}\n",
    "\\newcommand{\\params}{\\mathbf{\\theta}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Model\n",
    "\n",
    "Consider a linear model that produces target outputs, $\\soutput$, by linearly  combining the model parameters, $\\weight_i$, and the input variables, $\\sinput_i$:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\soutput\\brac{\\vinput, \\weights} = \\weight_0 + \\weight_1\\sinput_1 + \\ldots +\n",
    "                                     \\weight_D\\sinput_D.\n",
    "\\end{equation}\n",
    "\n",
    "The linear model can be extended to produce outputs that are a linear combination of fixed nonlinear functions of the input variables. These functions are known as *basis* functions. The linear model can now be\n",
    "expressed as\n",
    "\n",
    "\\begin{equation}\n",
    "  \\soutput\\brac{\\vinput, \\weights} = \\weight_0\\sbasis_0\\brac{x} + \n",
    "                                     \\weight_1\\sbasis_1\\brac{\\sinput} + \n",
    "                                     \\ldots +\n",
    "                                     \\weight_M\\sbasis_M\\brac{\\sinput}\n",
    "\\end{equation}\n",
    "\n",
    "For a set of observations,\n",
    "\n",
    "\\begin{equation}\n",
    "  \\voutput = \\design\\weights\n",
    "\\end{equation}\n",
    "\n",
    "where:\n",
    "* $\\voutput = \\brac{\\soutput_1, \\ldots, \\soutput_N}^T$ are the observed scalar outputs.\n",
    "* $\\minput = \\brac{\\vinput_1, \\ldots, \\vinput_N}^T$ are the $M$-dimensional inputs.\n",
    "* $\\weights = \\brac{\\weight_1, \\ldots, \\weight_M}^T$ is the vector of model parameters.\n",
    "* $\\design$ is the design matrix where the input data, $\\minput$, has been passed through the vector of nonlinear basis functions $\\vbasis = \\brac{\\sbasis_1, \\ldots, \\sbasis_D}$.\n",
    "\n",
    "The dimensions involved in the linear model are shown below\n",
    "\n",
    "\\begin{equation}\n",
    "   \\begin{bmatrix}\n",
    "     \\soutput_1 \\\\\n",
    "     \\vdots     \\\\\n",
    "     \\soutput_N \\\\\n",
    " \\end{bmatrix}\n",
    " =\n",
    " \\begin{bmatrix}\n",
    "     \\sbasis_0\\!\\brac{\\vinput_1} &  \\ldots & \\sbasis_D\\!\\brac{\\vinput_1} \\\\\n",
    "     \\vdots                      &  \\ddots & \\vdots                      \\\\\n",
    "     \\sbasis_0\\!\\brac{\\vinput_N} &  \\ldots & \\sbasis_D\\!\\brac{\\vinput_N} \\\\\n",
    " \\end{bmatrix}\n",
    " \\begin{bmatrix}\n",
    "     \\weight_0 \\\\\n",
    "     \\vdots    \\\\\n",
    "     \\weight_D \\\\\n",
    " \\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "<a id='ml_example'></a>\n",
    "\n",
    "The example shown below is created by sampling from the uniform distribution $U\\brac{\\vinput|-1, 1}$, substituting these values into the function $f\\brac{\\vinput} = -0.3 + 0.5\\vinput$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create hidden linear model.\n",
    "w_true = [-0.3, 0.5]\n",
    "polybasis = lambda x, p: PolynomialFeatures(p).fit_transform(x)\n",
    "linear_model = lambda x, w=w_true: polybasis(x, len(w) - 1).dot(w).reshape(len(x), 1)\n",
    "\n",
    "utils.plot(({'x': np.linspace(-1., 1.)[:, None], 'model': linear_model},))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares parameter estimation\n",
    "<a id='least_squares'></a>\n",
    "\n",
    "Supposing the form of the model is *known* but the model parameters are *unknown*. Following from the example above, we assume the model takes the form\n",
    "\n",
    "\\begin{equation}\n",
    "  f\\brac{\\vinput} = b + m\\vinput\n",
    "\\end{equation}\n",
    "\n",
    "Although the form of the model is known, the model parameters are treated as unknown. The task is to estimate the slope $m$ and intercept $b$ from the data. In this scenario we will also assume the data is perturbed by an unknown noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make noisy observations of model.\n",
    "N = 1000\n",
    "noise = 0.2\n",
    "X = np.random.uniform(-1.0, 1.0, size=(N, 1)).reshape((N, 1))\n",
    "y = linear_model(X) + np.random.normal(scale=noise, size=(N, 1)) \n",
    "\n",
    "utils.plot({'x': X, 'y': y, 'linestyle': '', 'marker': '.', 'markersize': 2, 'color': 'k'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More generally, the task is to estimate the model weights $\\weights$ from the linear model:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\vtarget = \\design\\weights + \\vnoise\n",
    "\\end{equation}\n",
    "\n",
    "The least-squares solution can be obtained by finding the parameters that minimise the squared error between the model and observed values. These parameters are given by:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\weights_{ml} = (\\design^T\\design)^{-1}\\design^T\\vtarget\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Note that numpy provides a function for implementing the above equation.   \n",
    "# 'lstsq' returns the least-squares solution to a linear matrix equation.\n",
    "w_ml = np.linalg.lstsq(polybasis(X, len(w_true)), y)[0]\n",
    "model = lambda x: linear_model(x, w=w_ml)\n",
    "\n",
    "utils.plot(({'x': X, 'y': y, 'linestyle': '', 'marker': '.', 'markersize': 2, 'color': 'k'},\n",
    "            {'x': X, 'model': model, 'color': 'r', 'linewidth': 2}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"'True' coefficients:        {}\".format(w_true)\n",
    "print \"Least-squares coefficients: {}\".format(w_ml.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Linear model\n",
    "\n",
    "If the observations are perturbed by independent and identically distributed (i.i.d.) Gaussian noise with a zero mean and a precision of $\\precision$, an observed output can be modelled as a Gaussian random variable where\n",
    "\n",
    "\\begin{equation}\n",
    "  \\starget = \\weights^T \\sbasisfcn{\\vinput} + \\vnoise,\n",
    "\\end{equation}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation}\n",
    "  \\condprob{\\starget}{\\vinput, \\weights, \\precision} = \n",
    "  \\normalcond{\\starget}{\\weights^T \\sbasisfcn{\\vinput}}{\\precision^{-1}}.\n",
    "\\end{equation}\n",
    "\n",
    "Assuming Gaussian noise, the above equation states that the expected output value at a nominated input location, $\\vinput$, is a function of the basis functions, $\\vbasis$, and model parameters, $\\weights$. The variance in observations is inversely proportional to the precision of the Gaussian, $\\precision^{-1}$. Often these variables are unknown. In such cases, fitting a linear model to the data requires finding the parameters $\\params = \\brac{\\weights, \\precision}$ that optimally model the relationship between the observed input and output data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter estimation\n",
    "\n",
    "The Bayesian approach to estimating the model parameters, $\\params$, is to place a distribution over these variables. For a Gaussian with an unknown mean and covariance, the normal-gamma distribution can be used.\n",
    "\n",
    "\\begin{equation}\n",
    "  \\prob{\\weights, \\precision} = \\normalgammacond{\\weights, \\precision}\n",
    "                                                {\\normalmean}{\\normalprecision}\n",
    "                                                {\\gammashape}{\\gammarate}\n",
    "\\end{equation}\n",
    "\n",
    "Since the normal-gamma distribution forms a conjugate prior, closed form expressions can be drived for the distribution updates where:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\condprob{\\weights, \\precision}{\\minput, \\voutput} =\n",
    "          \\normalgammacond{\\weights, \\precision}\n",
    "                          {\\normalmean_N}{\\normalprecision_N}\n",
    "                          {\\gammashape_N}{\\gammarate_N}\n",
    "\\end{equation}\n",
    "\n",
    "and the sufficient statistics of the update are given by:\n",
    "\n",
    "\\begin{align}\n",
    "  \\normalmean_N &= \\normalprecision_N\n",
    "                   \\brac{\\normalprecision^{-1}_0\\normalmean_0 +\n",
    "                         \\design^T\\voutput}\n",
    "  \\\\\n",
    "  \\normalprecision^{-1}_N &= \\normalprecision^{-1}_0 + \\design^T\\design\n",
    "  \\\\\n",
    "  \\gammashape_N &= \\gammashape_0 + \\frac{n}{2}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{equation}\n",
    "  \\gammarate_N = \\gammarate_0 + \\frac{k}{2}\n",
    "                 \\brac{\\normalmean_0^T\\normalprecision_0^{-1}\\normalmean_0 +\n",
    "                       \\voutput^T\\voutput -\n",
    "                       \\normalmean_N^T\\normalprecision_N^{-1}\\normalmean_N}\n",
    "\\end{equation}\n",
    "\n",
    "These equations allow the distribution over the model parameters to be updated sequentially as data is observed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "<a id='bayesian_updates'></a>\n",
    "\n",
    "This section replicates the examples shown in Figure 3.7 of [1] and Figure 7.11 of [2]. The example has been extended to the case where the precision is unknown. The example demonstrates sequential Bayesian updating using a simple two parameter model of the form $\\voutput\\brac{\\vinput, \\weights} = w_0 + w_1\\vinput$. Since this model only has two parameters, the posterior can be visualised easily. \n",
    "\n",
    "As in the previous [example](#ml_example), the input data is created by sampling from the uniform distribution $U\\brac{\\vinput|-1, 1}$. The input data is substituted into the function $f\\brac{\\vinput} = -0.3 + 0.5\\vinput$ and Gaussian noise, with a standard deviation of 0.2, is added to the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation\n",
    "The distribution is initialised with an uninformative and improper prior where:\n",
    "\n",
    "\\begin{align}\n",
    "  \\normalmean_0 &= \\mathbf{0}                     \\\\\n",
    "  \\normalprecision^{-1} &= \\infty^{-1}\\mathbf{I}  \\\\\n",
    "  \\gammashape_0 &= \\frac{-D}{2}                   \\\\\n",
    "  \\gammarate_0 &= 0                               \\\\\n",
    "\\end{align}\n",
    "\n",
    "Since the prior is improper, it is only defined for $D < N - 1$. In the 2D example, the model can only be updated using more than three observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create Bayesian linear model.\n",
    "basis = lambda x: polybasis(x, 1)\n",
    "blm = BayesianLinearModel(basis=basis, dispersion=100*eye(2), shape=0)\n",
    "\n",
    "# Perform update.\n",
    "blm.update(X[:2], y[:2])\n",
    "utils.plot_update(X[:4], y[:4], linear_model, blm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blm.update(X[4:10], y[4:10])\n",
    "utils.plot_update(X[:10], y[:10], linear_model, blm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 100 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blm.update(X[10:100], y[10:100])\n",
    "utils.plot_update(X[:100], y[:100], linear_model, blm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1000 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blm.update(X[100:], y[100:])\n",
    "utils.plot_update(X, y, linear_model, blm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Note that as more observations are added, the posterior distribution collapses on the correct estimates of the model parameters. Since an uniformative prior was used, the maximum a posteriori estimates of the model weights are identical to the maximum likelihood solution. This can be confirmed by comparing the Bayesian solution to the maximum likelihood solution:\n",
    "<a id='equivalence'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"'True' coefficients:        {}\".format(w_true)\n",
    "print \"Least-squares coefficients: {}\".format(w_ml.squeeze())\n",
    "print \"Bayesian coefficients:      {}\".format(blm.location.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Model Selection\n",
    "<a id='model_selection'></a>\n",
    "\n",
    "It is important to note that the term 'linear model' describes linearity in the model parameters. That is, the output is a linear combination of the model weights and the inputs (or design matrix produced by basis function expansion). For example, complex non-linear outputs can be constructed using high degree polynomial basis functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 50\n",
    "noise = 0.25\n",
    "X = np.sort(np.random.uniform(0, 2*np.pi, N)).reshape((N, 1))\n",
    "y = np.sin(X) + np.random.normal(scale=noise, size=(N, 1)) \n",
    "\n",
    "# Approximate Sin function with a 6-degree polynomial.\n",
    "w_ml = np.linalg.lstsq(polybasis(X, 6), y)[0]\n",
    "model = lambda x: linear_model(x, w=w_ml)\n",
    "\n",
    "utils.plot(({'x': X, 'y': y, 'linestyle': '', 'marker': '.', 'markersize': 2, 'color': 'k'},\n",
    "            {'x': X, 'model': model, 'color': 'r', 'linewidth': 2}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model is known, the parameters can be estimated using [least-squares](#least_squares) or [Bayesian updates](#bayesian_updates). The [previous section](#equivalence) showed that under a (semi-conjugate) uninformative prior, these two methods produce the same estimates. As the complexity of the data increases, specifying a model may be difficult.\n",
    "\n",
    "Bayesian methods have a distinct advantage over maximum-likelihood methods during model selection. The problem with using maximum likelihood for model selection is that it favours complex models. As the complexity of the model is increased, the extra degrees of freedom allow the model to more precisely fit the training data. Despite producing models which reflect the training data accurately, the models usually have poor generalisation and reflect noise in the data - these models are examples of 'over-fitting'.\n",
    "\n",
    "Bayesian methods differ to maximum likelihood model selection in that they display a preference for simpler models. Bayesian model selection is performed by calculating the marginal likelihood also known as the model evidence. For a specified model, this is done by marginalising the model parameters and calculating the probability of the data:\n",
    "\n",
    "\\begin{equation}\n",
    "   \\condprob{\\data}{m} = \\int \\condprob{\\data}{\\params}\\condprob{\\theta}{m}d\\theta\n",
    "\\end{equation}\n",
    "\n",
    "For the Bayesian linear model, the marginal likelihood is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\prob{\\data} = \\frac{1}{2\\pi^\\frac{D}{2}}\n",
    "                   \\frac{{\\gammascale_0}^{\\gammashape_0}}\n",
    "                        {{\\gammascale_N}^{\\gammashape_N}}\n",
    "                   \\frac{\\gammafcn{\\gammashape_N}}\n",
    "                        {\\gammafcn{\\gammashape_0}}\n",
    "                   \\frac{\\det{\\normalprecision_N}^\\frac{1}{2}}\n",
    "                        {\\det{\\normalprecision_0}^\\frac{1}{2}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "In this example, polynomials of increasing complexity are fit to the trigonometric data shown in the [previous figure](#model_selection). Trigonometric functions are complex non-linear functions which cannot be exactly represented using a linear model. Despite this, a polynomial of sufficient complexity can reasonably approximate the observed data. This leaves the open question: how many degrees are sufficient? Model selection can be used to answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_plots = 15\n",
    "num_cols = 3\n",
    "num_rows = np.ceil(float(num_plots)/num_cols)\n",
    "\n",
    "# Interate through polynomial degrees and plot linear models.\n",
    "MSE = np.zeros(num_plots)\n",
    "ML = np.zeros(num_plots)\n",
    "fig = plt.figure(figsize=(5 * num_cols, 5 * num_rows))\n",
    "fig.subplots_adjust(hspace=0.6)\n",
    "for p in range(0, num_plots):\n",
    "\n",
    "    # Determine the maximum likelihood weights and evaluate the model.\n",
    "    theta = polybasis(X, p)    \n",
    "    w_ml = np.linalg.lstsq(theta, y)[0]\n",
    "    fQuery = theta.dot(w_ml)\n",
    "    MSE[p] = np.mean((fQuery - y)**2)\n",
    "    \n",
    "    # Create Bayesian linear model.\n",
    "    blm = BayesianLinearModel(basis=lambda x: polybasis(x, p))\n",
    "    blm.update(X, y)\n",
    "    ML[p] = blm.evidence()\n",
    "    \n",
    "    # Plot ML/Bayesian linear models.\n",
    "    plt.subplot(num_rows, num_cols, p + 1)\n",
    "    utils.plot(({'x': X, 'y': y, 'linestyle': '', 'marker': '.', 'markersize': 2, 'color': 'k'},\n",
    "                {'x': X, 'y': fQuery, 'color': 'b', 'linestyle': '--', 'linewidth': 2},\n",
    "                {'x': X, 'y': blm.predict(X), 'color': 'r', 'linewidth': 2}))\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('f(x)')\n",
    "    plt.title('Polynomial degrees: ' + str(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, note that if an uninformative prior is used, the Bayesian solution is identical to the maximum likelihood solution (least-squares). The plots above show that as the model complexity increases, the model provides a closer fit to the data.\n",
    "\n",
    "The mean squared error decreases as the model complexity increases. This is shown in the figure below. This behaviour heavily biases the maximum liklihood method towards over-fitting. Note that the rate at which the mean square error decreases has plateaued by three degrees of freedom. Conversely, the log marginal likelihood, returned by the Bayesian solution, displays a distinct peak at three degrees of freedom. Beyond three degrees of freedom, model complexity is penalised more heavily than data-fit, leading to deminishing log marginal likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "utils.plot_model_fit(MSE, ML)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
